{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection using Open CV, CNN and VGG-16 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.utils import  np_utils\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create emotion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emotions_dataset(category, emotion_code):\n",
    "    \n",
    "    currFrame=0\n",
    "    while(True):\n",
    "\n",
    "        if currFrame>300:\n",
    "            break\n",
    "            \n",
    "        #ret holds boolean value if read correctly or not\n",
    "        ret, frame= cam.read()\n",
    "        cv2.imwrite('data1/'+category+'/'+emotion_code+ str(currFrame) + '.jpg', frame)\n",
    "\n",
    "        currFrame= currFrame +1\n",
    "\n",
    "        #waitkey 1 will wait for keyPress for just 1 millisecond and it will continue to refresh and read frame\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_emotions_dataset('Neutral','ne')\n",
    "#create_emotions_dataset('Angry','an')\n",
    "#create_emotions_dataset('Sad','sa')\n",
    "create_emotions_dataset('Smile','sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Angry', 'Neutral', 'Sad', 'Smile']\n",
      "{'Angry': 0, 'Neutral': 1, 'Sad': 2, 'Smile': 3}\n"
     ]
    }
   ],
   "source": [
    "categories= os.listdir('data1')\n",
    "\n",
    "labels=[i for i in range(len(categories))]\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "label_dict=dict(zip(categories,labels)) #empty dictionary\n",
    "\n",
    "print(categories)\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angry has  300  images\n",
      "Neutral has  300  images\n",
      "Sad has  300  images\n",
      "Smile has  300  images\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "target=[]\n",
    "img_size=100\n",
    "\n",
    "for category in categories:\n",
    "    images= os.listdir('data1/'+category)\n",
    "    print(category,\"has \",len(images),\" images\")\n",
    "\n",
    "    for image in images:\n",
    "        multi= 0\n",
    "        img_path = 'data1/'+category+'/'+ image\n",
    "        img= cv2.imread(img_path)\n",
    "\n",
    "        #it will only be counted as a valid face if the number of responses for this face is higher than minNeighbors\n",
    "        faces = face_cascade.detectMultiScale(img, minNeighbors=10, minSize=(64,64))\n",
    "\n",
    "        try:\n",
    "            for (x,y,w,h) in faces:\n",
    "                multi = multi +1\n",
    "                sub_face = img[y:y + h, x:x + w]\n",
    "                cv2.imwrite('dataset1/'+category+'_face/'+ image, sub_face)\n",
    "\n",
    "                gray=cv2.cvtColor(sub_face,cv2.COLOR_BGR2GRAY)           \n",
    "                resized=cv2.resize(gray,(img_size,img_size))\n",
    "                data.append(resized)\n",
    "                target.append(label_dict[category])\n",
    "            \n",
    "            if multi !=1:\n",
    "                print(str(multi) + \" \"+ image)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print('Exception:',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    300\n",
       "2    300\n",
       "1    300\n",
       "0    300\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(target).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 100, 100)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 84,  76,  74, ...,  69,  74, 112],\n",
       "       [ 75,  70,  70, ...,  69,  69, 102],\n",
       "       [ 67,  66,  66, ...,  66,  74, 103],\n",
       "       ...,\n",
       "       [ 72,  66,  60, ...,  42,  44,  44],\n",
       "       [ 73,  62,  58, ...,  44,  45,  45],\n",
       "       [ 66,  61,  57, ...,  42,  46,  47]], dtype=uint8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see the first image\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32941176, 0.29803922, 0.29019608, ..., 0.27058824, 0.29019608,\n",
       "        0.43921569],\n",
       "       [0.29411765, 0.2745098 , 0.2745098 , ..., 0.27058824, 0.27058824,\n",
       "        0.4       ],\n",
       "       [0.2627451 , 0.25882353, 0.25882353, ..., 0.25882353, 0.29019608,\n",
       "        0.40392157],\n",
       "       ...,\n",
       "       [0.28235294, 0.25882353, 0.23529412, ..., 0.16470588, 0.17254902,\n",
       "        0.17254902],\n",
       "       [0.28627451, 0.24313725, 0.22745098, ..., 0.17254902, 0.17647059,\n",
       "        0.17647059],\n",
       "       [0.25882353, 0.23921569, 0.22352941, ..., 0.16470588, 0.18039216,\n",
       "        0.18431373]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize the entrie data\n",
    "data= data/255.0\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 100, 100, 1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare the data shape for the CNN model\n",
    "data = np.reshape(data, (data.shape[0], img_size, img_size, 1))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "target= np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    248\n",
       "1    239\n",
       "3    238\n",
       "0    235\n",
       "dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.2, random_state=1)\n",
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "#y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "kernel_size=(3,3)\n",
    "\n",
    "model.add(Conv2D(filters= 32, kernel_size= kernel_size ,activation='relu', input_shape= (img_size,img_size,1)))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(filters= 64, kernel_size= kernel_size ,activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(filters= 64, kernel_size= kernel_size ,activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(filters= 32, kernel_size= kernel_size ,activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 768 samples, validate on 192 samples\n",
      "Epoch 1/15\n",
      "768/768 [==============================] - 9s 11ms/step - loss: 1.4032 - accuracy: 0.2643 - val_loss: 1.3857 - val_accuracy: 0.2604\n",
      "Epoch 2/15\n",
      "768/768 [==============================] - 8s 10ms/step - loss: 1.3785 - accuracy: 0.2995 - val_loss: 1.3801 - val_accuracy: 0.2604\n",
      "Epoch 3/15\n",
      "768/768 [==============================] - 8s 10ms/step - loss: 1.3419 - accuracy: 0.3581 - val_loss: 1.3399 - val_accuracy: 0.2344\n",
      "Epoch 4/15\n",
      "768/768 [==============================] - 8s 11ms/step - loss: 1.1748 - accuracy: 0.4896 - val_loss: 0.9499 - val_accuracy: 0.9896\n",
      "Epoch 5/15\n",
      "768/768 [==============================] - 8s 10ms/step - loss: 0.6468 - accuracy: 0.7578 - val_loss: 0.2258 - val_accuracy: 0.9896\n",
      "Epoch 6/15\n",
      "768/768 [==============================] - 8s 10ms/step - loss: 0.2699 - accuracy: 0.9062 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "768/768 [==============================] - 8s 10ms/step - loss: 0.1152 - accuracy: 0.9609 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "768/768 [==============================] - 8s 10ms/step - loss: 0.1173 - accuracy: 0.9661 - val_loss: 0.0481 - val_accuracy: 0.9792\n",
      "Epoch 9/15\n",
      "768/768 [==============================] - 8s 10ms/step - loss: 0.1214 - accuracy: 0.9531 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "768/768 [==============================] - 8s 10ms/step - loss: 0.0460 - accuracy: 0.9857 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "768/768 [==============================] - 8s 10ms/step - loss: 0.0188 - accuracy: 0.9961 - val_loss: 8.8192e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "768/768 [==============================] - 8s 11ms/step - loss: 0.0107 - accuracy: 0.9974 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "768/768 [==============================] - 8s 11ms/step - loss: 0.0288 - accuracy: 0.9948 - val_loss: 8.8712e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "768/768 [==============================] - 8s 11ms/step - loss: 0.0128 - accuracy: 0.9974 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "768/768 [==============================] - 8s 11ms/step - loss: 0.0107 - accuracy: 0.9974 - val_loss: 2.4123e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1c0d08e89b0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, epochs=15, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 0, 3, 0, 2, 1, 1, 2, 0, 0, 1, 0, 2, 1, 3, 0, 1, 3, 3, 1,\n",
       "       3, 1, 0, 2, 1, 0, 3, 1, 3, 3, 3, 2, 0, 0, 1, 3, 3, 0, 1, 1, 0, 0,\n",
       "       2, 1, 2, 3, 2, 1, 3, 1, 2, 0, 1, 2, 1, 1, 1, 3, 1, 0, 0, 2, 0, 1,\n",
       "       3, 3, 3, 3, 1, 2, 1, 3, 0, 3, 0, 0, 2, 1, 3, 1, 0, 1, 1, 0, 2, 2,\n",
       "       2, 1, 3, 1, 3, 2, 0, 3, 0, 2, 2, 1, 3, 2, 3, 1, 1, 0, 1, 1, 0, 3,\n",
       "       3, 2, 3, 0, 3, 0, 3, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0, 1, 0, 2, 2, 3,\n",
       "       1, 0, 0, 2, 0, 3, 0, 0, 2, 1, 3, 3, 0, 3, 3, 2, 1, 3, 2, 1, 2, 1,\n",
       "       3, 2, 0, 1, 0, 2, 2, 0, 3, 1, 0, 3, 1, 0, 0, 2, 3, 2, 1, 2, 3, 3,\n",
       "       2, 2, 0, 1, 2, 3, 0, 3, 1, 1, 1, 2, 2, 3, 3, 3, 1, 0, 1, 2, 3, 0,\n",
       "       3, 2, 0, 2, 3, 0, 2, 1, 0, 0, 3, 2, 1, 0, 0, 0, 0, 0, 3, 1, 0, 1,\n",
       "       3, 2, 0, 2, 0, 1, 1, 3, 3, 1, 3, 2, 3, 3, 0, 0, 2, 3, 2, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred= model.predict(x_test)\n",
    "\n",
    "y_classes = y_pred.argmax(axis=-1)\n",
    "\n",
    "y_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 0, 3, 0, 2, 1, 1, 2, 0, 0, 1, 0, 2, 1, 3, 0, 1, 3, 3, 1,\n",
       "       3, 1, 0, 2, 1, 0, 3, 1, 3, 3, 3, 2, 0, 0, 1, 3, 3, 0, 1, 1, 0, 0,\n",
       "       2, 1, 2, 3, 2, 1, 3, 1, 2, 0, 1, 2, 1, 1, 1, 3, 1, 0, 0, 2, 0, 1,\n",
       "       3, 3, 3, 3, 1, 2, 1, 3, 0, 3, 0, 0, 2, 1, 3, 1, 0, 1, 1, 0, 2, 2,\n",
       "       2, 1, 3, 1, 3, 2, 0, 3, 0, 2, 2, 1, 3, 2, 3, 1, 1, 0, 1, 1, 0, 3,\n",
       "       3, 2, 3, 0, 3, 0, 3, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0, 1, 0, 2, 2, 3,\n",
       "       1, 0, 0, 2, 0, 3, 0, 0, 2, 1, 3, 3, 0, 3, 3, 2, 1, 3, 2, 1, 2, 1,\n",
       "       3, 2, 0, 1, 0, 2, 2, 0, 3, 1, 0, 3, 1, 0, 0, 2, 3, 2, 1, 2, 3, 3,\n",
       "       2, 2, 0, 1, 2, 3, 0, 3, 1, 1, 1, 2, 2, 3, 3, 3, 1, 0, 1, 2, 3, 0,\n",
       "       3, 2, 0, 2, 3, 0, 2, 1, 0, 0, 3, 2, 1, 0, 0, 0, 0, 0, 3, 1, 0, 1,\n",
       "       3, 2, 0, 2, 0, 1, 1, 3, 3, 1, 3, 2, 3, 3, 0, 0, 2, 3, 2, 1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_m = np_utils.to_categorical(y_test)\n",
    "score= model.evaluate(x_test,y_test_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00030502786103170365, 1.0]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "dict_opp={0:'Angry',1: 'Neutral',2:'Sad',3:'Smile'}\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(frame,minNeighbors=10, minSize=(64,64))    \n",
    "    try:\n",
    "            for (x,y,w,h) in faces:\n",
    "                sub_face = frame[y:y + h, x:x + w]\n",
    "                gray=cv2.cvtColor(sub_face,cv2.COLOR_BGR2GRAY)           \n",
    "\n",
    "                resized=cv2.resize(gray,(img_size,img_size))\n",
    "                normalized=resized/255.0\n",
    "                reshaped=np.reshape(normalized,(1,img_size,img_size,1))\n",
    "\n",
    "                result = model.predict(reshaped)\n",
    "                y_class = np.argmax(result,axis=1)[0]\n",
    "                    \n",
    "                cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                cv2.putText(frame, dict_opp[y_class], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "\n",
    "                \n",
    "    except Exception as e:\n",
    "            print('Exception:',e)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1204 images belonging to 4 classes.\n",
      "Epoch 1/10\n",
      "38/38 [==============================] - 13s 341ms/step - loss: 1.3924 - accuracy: 0.2375\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 13s 350ms/step - loss: 1.3744 - accuracy: 0.2965\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 13s 349ms/step - loss: 1.2172 - accuracy: 0.4502\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 13s 344ms/step - loss: 0.8231 - accuracy: 0.6537\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 13s 334ms/step - loss: 0.4127 - accuracy: 0.8389\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 13s 330ms/step - loss: 0.2041 - accuracy: 0.9336\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 13s 331ms/step - loss: 0.2562 - accuracy: 0.8970\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 12s 326ms/step - loss: 0.1806 - accuracy: 0.9435\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 12s 328ms/step - loss: 0.0845 - accuracy: 0.9709\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 13s 330ms/step - loss: 0.0988 - accuracy: 0.9726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1c0c6202828>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset1',\n",
    "                                                 target_size = (img_size, img_size),\n",
    "                                                 batch_size = 32,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "\n",
    "# fit the model\n",
    "model.fit_generator(\n",
    "  training_set,\n",
    "  epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "dict_opp={0:'Angry',1: 'Neutral',2:'Sad',3:'Smile'}\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(frame,minNeighbors=10, minSize=(64,64))    \n",
    "    try:\n",
    "            for (x,y,w,h) in faces:\n",
    "                sub_face = frame[y:y + h, x:x + w]\n",
    "                gray=cv2.cvtColor(sub_face,cv2.COLOR_BGR2GRAY)           \n",
    "\n",
    "                resized=cv2.resize(gray,(img_size,img_size))\n",
    "                normalized=resized/255.0\n",
    "                reshaped=np.reshape(normalized,(1,img_size,img_size,1))\n",
    "\n",
    "                result = model.predict(reshaped)\n",
    "                y_class = np.argmax(result,axis=1)[0]\n",
    "                    \n",
    "                cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                cv2.putText(frame, dict_opp[y_class], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "\n",
    "                \n",
    "    except Exception as e:\n",
    "            print('Exception:',e)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not include top layer that was built for imagenet classification\n",
    "vgg = VGG16(input_shape=[224,224,3] , weights='imagenet', include_top=False)\n",
    "\n",
    "# don't train existing weights\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(vgg.output)\n",
    "#x = Dense(128, activation='relu')(x)\n",
    "prediction = Dense(len(categories), activation='softmax')(x)\n",
    "\n",
    "# create a model object\n",
    "vgg_m = Model(inputs=vgg.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4)                 100356    \n",
      "=================================================================\n",
      "Total params: 14,815,044\n",
      "Trainable params: 100,356\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_m.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1204 images belonging to 4 classes.\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 274s 7s/step - loss: 0.3920 - accuracy: 0.8563\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 275s 7s/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 287s 8s/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 288s 8s/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 298s 8s/step - loss: 0.0063 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1c1012110f0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size=224\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset1',\n",
    "                                                 target_size = (img_size, img_size),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "\n",
    "# fit the model\n",
    "vgg_m.fit_generator(\n",
    "  training_set,\n",
    "  epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spotify Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>length</th>\n",
       "      <th>danceability</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>The Odyssey</td>\n",
       "      <td>223424</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.6940</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-16.362</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>78.662</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Feel It Still</td>\n",
       "      <td>162092</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.0907</td>\n",
       "      <td>0.758</td>\n",
       "      <td>-5.157</td>\n",
       "      <td>0.0624</td>\n",
       "      <td>79.088</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Another One of Those Days</td>\n",
       "      <td>246041</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.4490</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0851</td>\n",
       "      <td>0.375</td>\n",
       "      <td>-9.365</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>134.966</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>10,000 Hours (with Justin Bieber)</td>\n",
       "      <td>167693</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-4.644</td>\n",
       "      <td>0.0259</td>\n",
       "      <td>89.991</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Million Reasons</td>\n",
       "      <td>205280</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.4940</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-8.012</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>129.890</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name  length  danceability  acousticness  \\\n",
       "54                        The Odyssey  223424         0.252        0.6940   \n",
       "21                      Feel It Still  162092         0.799        0.0427   \n",
       "16          Another One of Those Days  246041         0.603        0.4490   \n",
       "67  10,000 Hours (with Justin Bieber)  167693         0.654        0.1530   \n",
       "31                    Million Reasons  205280         0.666        0.4940   \n",
       "\n",
       "    energy  instrumentalness  liveness  valence  loudness  speechiness  \\\n",
       "54   0.291          0.951000    0.1050    0.146   -16.362       0.0391   \n",
       "21   0.797          0.000075    0.0907    0.758    -5.157       0.0624   \n",
       "16   0.546          0.000000    0.0851    0.375    -9.365       0.0267   \n",
       "67   0.630          0.000000    0.1110    0.430    -4.644       0.0259   \n",
       "31   0.423          0.000000    0.1060    0.154    -8.012       0.0430   \n",
       "\n",
       "      tempo result  \n",
       "54   78.662   calm  \n",
       "21   79.088  happy  \n",
       "16  134.966    sad  \n",
       "67   89.991  happy  \n",
       "31  129.890    sad  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs= pd.read_csv('Spotify_songs.csv',index_col=0)\n",
    "songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feel It Still'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs[songs['result']=='happy'][['name']].iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "dict_opp={0:'Angry',1: 'Neutral',2:'Sad',3:'Smile'}\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(frame,minNeighbors=10, minSize=(64,64))    \n",
    "    try:\n",
    "            for (x,y,w,h) in faces:\n",
    "                sub_face = frame[y:y + h, x:x + w]\n",
    "\n",
    "                resized=cv2.resize(sub_face,(img_size,img_size))\n",
    "                normalized=resized/255.0\n",
    "                reshaped=np.reshape(normalized,(1,img_size,img_size,3))\n",
    "\n",
    "                result = vgg_m.predict(reshaped)\n",
    "                y_class = np.argmax(result,axis=1)[0]\n",
    "                    \n",
    "                cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                cv2.putText(frame, dict_opp[y_class], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "\n",
    "                # for demo purpose first song is taken , in actual random songs would be selected \n",
    "                # after the song is played for its duration(minutes)\n",
    "                if(dict_opp[y_class]=='Smile'):\n",
    "                    display_txt=songs[songs['result']=='happy'][['name']].iloc[0][0]\n",
    "                elif(dict_opp[y_class]=='Sad'):\n",
    "                    display_txt=songs[songs['result']=='sad'][['name']].iloc[0][0]   \n",
    "                elif(dict_opp[y_class]=='Neutral'):\n",
    "                    display_txt=songs[songs['result']=='neutral'][['name']].iloc[0][0]\n",
    "                elif(dict_opp[y_class]=='Angry'):\n",
    "                    display_txt=songs[songs['result']=='calm'][['name']].iloc[0][0]\n",
    "                    \n",
    "                cv2.putText(frame, display_txt + ' will be played', (x-100, y+200),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),2)\n",
    "                \n",
    "    except Exception as e:\n",
    "            print('Exception:',e)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
